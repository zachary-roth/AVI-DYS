{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVI-DYS Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from scipy.signal import savgol_filter\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specific Parameters (CHANGE THESE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "project_path = \"C:\\\\Users\\\\zacha\\\\Repos\\\\AVI-DYS\\\\AVI-DYS-lower-limb-zach_roth-2023-04-19\"\n",
    "save_path = \"C:\\\\Users\\\\zacha\\\\Data\\\\AVI-DYS\\\\Results\\\\Post-Processing\"\n",
    "\n",
    "IDs = ['001', '003', '004', '005', '115', '116', '117', '118', '120', '121',\n",
    "        '122', '123', '125', '126', '127', '128', '129', '130', '131', '132',\n",
    "        '133', '135', '138', '151', '152', '301', '303', '304', '305', '306',\n",
    "        '307', '308', '309']\n",
    "\n",
    "pcutoff = 0.8\n",
    "\n",
    "visualizations = True\n",
    "\n",
    "# Create the save paths - ! Make sure these folders exist!\n",
    "save_raw = os.path.join(save_path,'0-Raw')\n",
    "save_imp = os.path.join(save_path,'1-Imputed')\n",
    "save_filt = os.path.join(save_path,'2-Filtered')\n",
    "save_scale = os.path.join(save_path,'3-Scaled')\n",
    "save_norm = os.path.join(save_path,'4-Normalized')\n",
    "save_concat = os.path.join(save_path,'5-Concatenated')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df):\n",
    "    \"\"\"Fill gaps with sklearn's iterative imputer.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): raw data with missing values\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: data with imputed values\n",
    "    \"\"\"\n",
    "    imp = IterativeImputer(max_iter=100, random_state=42,sample_posterior=False,skip_complete=True)\n",
    "    cols = df.columns\n",
    "    imp.fit(df)\n",
    "    data = imp.transform(df)\n",
    "    df = pd.DataFrame(data,columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_mvmt_data(df,scale_factor):\n",
    "    \"\"\"Spatially normalize the the movement data by dividing all values by a scaling factor (the longest length of the KNE_ANK segment), then set the minimum value to 0.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Left or Right movement data\n",
    "        scale_factor (float): The maximum length of the KNE-ANK segment in pixels\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Spatially normalized (scaled) movement data\n",
    "    \"\"\"\n",
    "    df = df/scale_factor\n",
    "    cols = df.columns\n",
    "    for c in cols:\n",
    "        df[c] = df[c]-min(df[c])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_skltn_data(df,scale_factor):\n",
    "    \"\"\"Spatially normalize the skeleton data by dividing the segment lengths by a scaling factor (the longest length of the KNE_ANK segment).\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Left or Right movement data\n",
    "        scale_factor (float): The maximum length of the KNE-ANK segment in pixels\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Spatially normalized (scaled) skeleton data\n",
    "    \"\"\"\n",
    "    cols = df.columns\n",
    "    for c in cols:\n",
    "        if 'orientation' in c:\n",
    "            continue\n",
    "        else:\n",
    "            df[c] = df[c]/scale_factor\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_time(df):\n",
    "    \"\"\"Make all trials 60s seconds (3000 frames / 50fps) long. If they are longer than 60s, trim them, if they are shorter than 60s, mirror the trial and concat it. \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Left or Right movement or skeleton data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Temporally normalized movment or skeleton data\n",
    "    \"\"\"\n",
    "    while len(df) < 3000:\n",
    "        df = pd.concat([df,df[::-1]])\n",
    "    df = df.reset_index(drop=True)\n",
    "    if len(df) > 3000:\n",
    "        df = df.iloc[0:3000,]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lists of Common Variables\n",
    "\n",
    "# Create multindexes for column names\n",
    "bodyparts = ['RKNE', 'RANK', 'RD3M', 'RD1M', 'RHLX', \n",
    "             'LHLX', 'LD1M', 'LD3M', 'LANK', 'LKNE']\n",
    "iterables = [bodyparts, ['x','y','likelihood']]\n",
    "mvmt_cols = pd.MultiIndex.from_product(iterables, names=[\"bodyparts\", \"coords\"])\n",
    "\n",
    "segments = ['RKNE_RANK', 'RANK_RD3M', 'RANK_RD1M', 'RD1M_RHLX',\n",
    "            'LKNE_LANK', 'LANK_LD3M', 'LANK_LD1M', 'LD1M_LHLX']\n",
    "iterables = [segments, ['length','orientation','likelihood']]\n",
    "skltn_cols = pd.MultiIndex.from_product(iterables, names=[\"segments\", \"coords\"])\n",
    "\n",
    "# Create list of bodyparts per side\n",
    "left_bodyparts = ['LHLX', 'LD1M', 'LD3M', 'LANK', 'LKNE']\n",
    "right_bodyparts = ['RKNE', 'RANK', 'RD3M', 'RD1M', 'RHLX']\n",
    "\n",
    "# Create a list of left side body part columns in the same order as the right side\n",
    "left_mvmt_cols_reordered = ['LKNE_x', 'LKNE_y', 'LANK_x', 'LANK_y', 'LD3M_x', 'LD3M_y', \n",
    "                          'LD1M_x', 'LD1M_y', 'LHLX_x', 'LHLX_y']\n",
    "\n",
    "left_segments = ['LKNE_LANK', 'LANK_LD3M', 'LANK_LD1M', 'LD1M_LHLX']\n",
    "right_segments = ['RKNE_RANK', 'RANK_RD3M', 'RANK_RD1M', 'RD1M_RHLX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data path (the video folder in the project path)\n",
    "data_path = os.path.join(project_path,'videos')\n",
    "\n",
    "# Get a list of file names\n",
    "data_file_names = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "\n",
    "# Iterate over the participants\n",
    "for i in IDs:\n",
    "\n",
    "    # Get the paths to the left and right mvmt and skltn data\n",
    "    for n in data_file_names:\n",
    "        if i+'-12' in n and 'filtered.csv' in n:\n",
    "            left_mvmt_path = os.path.join(data_path,n)\n",
    "        elif  i+'-11' in n and 'filtered.csv' in n:\n",
    "            right_mvmt_path = os.path.join(data_path,n)\n",
    "        elif i+'-12' in n and 'filtered_skeleton.csv' in n:\n",
    "            left_skltn_path = os.path.join(data_path,n)\n",
    "        elif i+'-11' in n and 'filtered_skeleton.csv' in n:\n",
    "            right_skltn_path = os.path.join(data_path,n)\n",
    "        elif i+'-12' in n and 'labeled' in n:\n",
    "            left_video_path = os.path.join(data_path,n)\n",
    "            \n",
    "    # Read in the movement and skeleton data\n",
    "    mvmt_left = pd.read_csv(left_mvmt_path,index_col=0,names=mvmt_cols,skiprows=3)\n",
    "    mvmt_right = pd.read_csv(right_mvmt_path,index_col=0,names=mvmt_cols,skiprows=3)\n",
    "    skltn_left = pd.read_csv(left_skltn_path,index_col=0,names=skltn_cols,skiprows=2)\n",
    "    skltn_right = pd.read_csv(right_skltn_path,index_col=0,names=skltn_cols,skiprows=2)\n",
    "\n",
    "    # Rename columns\n",
    "    mvmt_left.columns = ['_'.join(col) for col in mvmt_left.columns]\n",
    "    mvmt_right.columns = ['_'.join(col) for col in mvmt_right.columns]\n",
    "    skltn_left.columns = ['_'.join(col) for col in skltn_left.columns]\n",
    "    skltn_right.columns = ['_'.join(col) for col in skltn_right.columns]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Apply p-cutoff and drop unused columns\n",
    "    # left movement data\n",
    "    for b in bodyparts:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        x_col = b+'_x'\n",
    "        y_col=b+'_y'\n",
    "        if b in right_bodyparts:\n",
    "            mvmt_left = mvmt_left.drop([likelihood_col,x_col,y_col],axis=1)\n",
    "        else:\n",
    "            mvmt_left.loc[mvmt_left[likelihood_col] < 0.8, [x_col, y_col]] = np.NaN\n",
    "            mvmt_left = mvmt_left.drop([likelihood_col],axis=1)\n",
    "            \n",
    "    # Reorder the left movement columns to the same order as the right side\n",
    "    mvmt_left = mvmt_left[left_mvmt_cols_reordered]\n",
    "\n",
    "    # right movement data\n",
    "    for b in bodyparts:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        x_col = b+'_x'\n",
    "        y_col=b+'_y'\n",
    "        if b in left_bodyparts:\n",
    "            mvmt_right = mvmt_right.drop([likelihood_col,x_col,y_col],axis=1)\n",
    "        else:\n",
    "            mvmt_right.loc[mvmt_right[likelihood_col] < 0.8, [x_col, y_col]] = np.NaN\n",
    "            mvmt_right = mvmt_right.drop([likelihood_col],axis=1)\n",
    "            \n",
    "    # left skeleton data\n",
    "    for b in segments:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        length_col = b+'_length'\n",
    "        orientation_col=b+'_orientation'\n",
    "        if b in right_segments:\n",
    "            skltn_left = skltn_left.drop([likelihood_col,length_col,orientation_col],axis=1)\n",
    "        else:\n",
    "            skltn_left.loc[skltn_left[likelihood_col] < 0.8, [length_col, orientation_col]] = np.NaN\n",
    "            skltn_left = skltn_left.drop([likelihood_col],axis=1)\n",
    "             \n",
    "    # right skeleton data\n",
    "    for b in segments:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        length_col = b+'_length'\n",
    "        orientation_col=b+'_orientation'\n",
    "        if b in left_segments:\n",
    "            skltn_right = skltn_right.drop([likelihood_col,length_col,orientation_col],axis=1)\n",
    "        else:\n",
    "            skltn_right.loc[skltn_right[likelihood_col] < 0.8, [length_col, orientation_col]] = np.NaN\n",
    "            skltn_right = skltn_right.drop([likelihood_col],axis=1)\n",
    "    \n",
    "    # Get Scale Factors for Spatial Normalization\n",
    "    scale_factor_left = max(skltn_left['LKNE_LANK_length'])\n",
    "    scale_factor_right = max(skltn_right['RKNE_RANK_length'])\n",
    "\n",
    "    # Mirror the left movement data\n",
    "    clip = VideoFileClip(os.path.join(data_path,left_video_path))\n",
    "    width = clip.w\n",
    "    for c in mvmt_left.columns:\n",
    "        if '_y' not in c:\n",
    "            mvmt_left[c] = abs(mvmt_left[c] - (width/2))\n",
    "\n",
    "    # Mirror the left segment orientations\n",
    "        for c in skltn_left.columns:\n",
    "            if 'length' not in c:\n",
    "                skltn_left[c] = 180 - skltn_left[c]\n",
    "    # Save the raw data\n",
    "    mvmt_left.to_csv(os.path.join(save_raw,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_raw,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_raw,f'{i}-skltn-left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_raw,f'{i}-skltn-right.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Interpolatate Missing Data with IterativeImputer\n",
    "    mvmt_left = fill_gaps(mvmt_left)\n",
    "    mvmt_right = fill_gaps(mvmt_right)\n",
    "    skltn_left = fill_gaps(skltn_left)\n",
    "    skltn_right = fill_gaps(skltn_right)\n",
    "\n",
    "    # Save the imputed data\n",
    "    mvmt_left.to_csv(os.path.join(save_imp,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_imp,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_imp,f'{i}-skltn-left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_imp,f'{i}-skltn-right.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Filter the data\n",
    "    for c in mvmt_left.columns:\n",
    "        mvmt_left[c] = savgol_filter(mvmt_left[c], 30, 2)\n",
    "\n",
    "    for c in mvmt_right.columns:\n",
    "        mvmt_right[c] = savgol_filter(mvmt_right[c], 30, 2)\n",
    "    \n",
    "    for c in skltn_left.columns:\n",
    "        skltn_left[c] = savgol_filter(skltn_left[c], 30, 2)\n",
    "\n",
    "    for c in skltn_right.columns:\n",
    "        skltn_right[c] = savgol_filter(skltn_right[c], 30, 2)\n",
    "\n",
    "    # Save the filtered data\n",
    "    mvmt_left.to_csv(os.path.join(save_filt,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_filt,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_filt,f'{i}-skltn-left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_filt,f'{i}-skltn-right.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Scale Data\n",
    "    mvmt_left = scale_mvmt_data(mvmt_left, scale_factor_left)\n",
    "    mvmt_right = scale_mvmt_data(mvmt_right, scale_factor_right)\n",
    "    skltn_left = scale_skltn_data(skltn_left, scale_factor_left)\n",
    "    skltn_right = scale_skltn_data(skltn_right, scale_factor_right)\n",
    "\n",
    "    # Save the scaled data\n",
    "    mvmt_left.to_csv(os.path.join(save_scale,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_scale,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_scale,f'{i}-skltn-left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_scale,f'{i}-skltn-right.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Temporally normalize the data\n",
    "    mvmt_left = normalize_time(mvmt_left)\n",
    "    mvmt_right = normalize_time(mvmt_right)\n",
    "    skltn_left = normalize_time(skltn_left)\n",
    "    skltn_right = normalize_time(skltn_right)\n",
    "\n",
    "    # Save the normalized data\n",
    "    mvmt_left.to_csv(os.path.join(save_norm,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_norm,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_norm,f'{i}-skltn-left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_norm,f'{i}-skltn-right.csv'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Concat left or right data together\n",
    "    cols = ['KNE_x', 'KNE_y', \n",
    "            'ANK_x', 'ANK_y', \n",
    "            'D3M_x', 'D3M_y', \n",
    "            'D1M_x', 'D1M_y', \n",
    "            'HLX_x', 'HLX_y', \n",
    "            'KNE_ANK_length', 'KNE_ANK_orientation', \n",
    "            'ANK_D3M_length', 'ANK_D3M_orientation',\n",
    "            'ANK_D1M_length', 'ANK_D1M_orientation', \n",
    "            'D1M_HLX_length', 'D1M_HLX_orientation',\n",
    "            ]\n",
    "   \n",
    "    left_data = pd.concat([mvmt_left,skltn_left],axis=1)\n",
    "    left_data.columns = cols\n",
    " \n",
    "    right_data = pd.concat([mvmt_right,skltn_right],axis=1)\n",
    "    right_data.columns = cols\n",
    "\n",
    "    # Save the concatenated data\n",
    "    left_data.to_csv(os.path.join(save_concat,f'{i}-left-data.csv'))\n",
    "    right_data.to_csv(os.path.join(save_concat,f'{i}-right-data.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sktime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
