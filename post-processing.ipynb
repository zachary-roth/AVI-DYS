{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVI-DYS Post Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn import preprocessing\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Specific Parameters (CHANGE THESE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "project_path = \"C:\\\\Users\\\\zacha\\\\Repos\\\\AVI-DYS\\\\AVI-DYS-lower-limb-zach_roth-2023-04-19\"\n",
    "save_path = \"C:\\\\Users\\\\zacha\\\\Data\\\\AVI-DYS\\\\Results\\\\Post-Processing\"\n",
    "\n",
    "IDs = ['152']\n",
    "\n",
    "pcutoff = 0.8\n",
    "\n",
    "visualizations = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(df):\n",
    "    \"\"\"Fill gaps with sklearn's iterative imputer.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): raw data with missing values\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: data with imputed values\n",
    "    \"\"\"\n",
    "    imp = IterativeImputer(max_iter=10, random_state=42,sample_posterior=True)\n",
    "    cols = df.columns\n",
    "    imp.fit(df)\n",
    "    data = imp.transform(df)\n",
    "    df = pd.DataFrame(data,columns=cols)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lists of Common Variables\n",
    "\n",
    "# Create multindexes for column names\n",
    "bodyparts = ['RKNE', 'RANK', 'RD3M', 'RD1M', 'RHLX', \n",
    "             'LHLX', 'LD1M', 'LD3M', 'LANK', 'LKNE']\n",
    "iterables = [bodyparts, ['x','y','likelihood']]\n",
    "mvmt_cols = pd.MultiIndex.from_product(iterables, names=[\"bodyparts\", \"coords\"])\n",
    "\n",
    "segments = ['RKNE_RANK', 'RANK_RD3M', 'RANK_RD1M', 'RD1M_RHLX',\n",
    "            'LKNE_LANK', 'LANK_LD3M', 'LANK_LD1M', 'LD1M_LHLX']\n",
    "iterables = [segments, ['length','orientation','likelihood']]\n",
    "skltn_cols = pd.MultiIndex.from_product(iterables, names=[\"segments\", \"coords\"])\n",
    "\n",
    "# Create list of bodyparts per side\n",
    "left_bodyparts = ['LHLX', 'LD1M', 'LD3M', 'LANK', 'LKNE']\n",
    "right_bodyparts = ['RKNE', 'RANK', 'RD3M', 'RD1M', 'RHLX']\n",
    "\n",
    "left_segments = ['LKNE_LANK', 'LANK_LD3M', 'LANK_LD1M', 'LD1M_LHLX']\n",
    "right_segments = ['RKNE_RANK', 'RANK_RD3M', 'RANK_RD1M', 'RD1M_RHLX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the save paths\n",
    "save_raw = os.path.join(save_path,'Raw')\n",
    "save_imp = os.path.join(save_path,'Imputed')\n",
    "\n",
    "# Create the data path (the video folder in the project path)\n",
    "data_path = os.path.join(project_path,'videos')\n",
    "\n",
    "# Get a list of file names\n",
    "data_file_names = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "\n",
    "# Iterate over the participants\n",
    "for i in IDs:\n",
    "    # Make the Save Folders\n",
    "    \n",
    "    # Get the paths to the left and right mvmt and skltn data\n",
    "    for n in data_file_names:\n",
    "        if i+'-11' in n and 'filtered.csv' in n:\n",
    "            left_mvmt_path = os.path.join(data_path,n)\n",
    "        elif  i+'-12' in n and 'filtered.csv' in n:\n",
    "            right_mvmt_path = os.path.join(data_path,n)\n",
    "        elif i+'-11' in n and 'filtered_skeleton.csv' in n:\n",
    "            left_skltn_path = os.path.join(data_path,n)\n",
    "        elif i+'-12' in n and 'filtered_skeleton.csv' in n:\n",
    "            right_skltn_path = os.path.join(data_path,n)\n",
    "            \n",
    "    # Read in the movement and skeleton data\n",
    "    mvmt_left = pd.read_csv(left_mvmt_path,index_col=0,names=mvmt_cols,skiprows=3)\n",
    "    mvmt_right = pd.read_csv(right_mvmt_path,index_col=0,names=mvmt_cols,skiprows=3)\n",
    "    skltn_left = pd.read_csv(left_skltn_path,index_col=0,names=skltn_cols,skiprows=2)\n",
    "    skltn_right = pd.read_csv(right_skltn_path,index_col=0,names=skltn_cols,skiprows=2)\n",
    "\n",
    "    # Rename columns\n",
    "    mvmt_left.columns = ['_'.join(col) for col in mvmt_left.columns]\n",
    "    mvmt_right.columns = ['_'.join(col) for col in mvmt_right.columns]\n",
    "    skltn_left.columns = ['_'.join(col) for col in skltn_left.columns]\n",
    "    skltn_right.columns = ['_'.join(col) for col in skltn_right.columns]\n",
    "    \n",
    "    # Apply p-cutoff and drop unused columns\n",
    "    \n",
    "    # left movement data\n",
    "    for b in bodyparts:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        x_col = b+'_x'\n",
    "        y_col=b+'_y'\n",
    "        if b in right_bodyparts:\n",
    "            mvmt_left = mvmt_left.drop([likelihood_col,x_col,y_col],axis=1)\n",
    "        else:\n",
    "            mvmt_left.loc[mvmt_left[likelihood_col] < 0.8, [x_col, y_col]] = np.NaN\n",
    "            mvmt_left = mvmt_left.drop([likelihood_col],axis=1)\n",
    "\n",
    "    # right movement data\n",
    "    for b in bodyparts:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        x_col = b+'_x'\n",
    "        y_col=b+'_y'\n",
    "        if b in left_bodyparts:\n",
    "            mvmt_right = mvmt_right.drop([likelihood_col,x_col,y_col],axis=1)\n",
    "        else:\n",
    "            mvmt_right.loc[mvmt_right[likelihood_col] < 0.8, [x_col, y_col]] = np.NaN\n",
    "            mvmt_right = mvmt_right.drop([likelihood_col],axis=1)\n",
    "            \n",
    "    # left skeleton data\n",
    "    for b in segments:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        length_col = b+'_length'\n",
    "        orientation_col=b+'_orientation'\n",
    "        if b in right_segments:\n",
    "            skltn_left = skltn_left.drop([likelihood_col,length_col,orientation_col],axis=1)\n",
    "        else:\n",
    "            skltn_left.loc[skltn_left[likelihood_col] < 0.8, [length_col, orientation_col]] = np.NaN\n",
    "            skltn_left = skltn_left.drop([likelihood_col],axis=1)\n",
    "             \n",
    "    # right skeleton data\n",
    "    for b in segments:\n",
    "        likelihood_col = b+'_likelihood'\n",
    "        length_col = b+'_length'\n",
    "        orientation_col=b+'_orientation'\n",
    "        if b in left_segments:\n",
    "            skltn_right = skltn_right.drop([likelihood_col,length_col,orientation_col],axis=1)\n",
    "        else:\n",
    "            skltn_right.loc[skltn_right[likelihood_col] < 0.8, [length_col, orientation_col]] = np.NaN\n",
    "            skltn_right = skltn_right.drop([likelihood_col],axis=1)\n",
    "    \n",
    "    # Get Scale Factors for Spatial Normalization\n",
    "    scale_factor_left = max(skltn_left['LKNE_LANK_length'])\n",
    "    scale_factor_right = max(skltn_right['RKNE_RANK_length'])\n",
    "\n",
    "    # Save the raw data\n",
    "    mvmt_left.to_csv(os.path.join(save_raw,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_raw,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_raw,f'{i}-skltn_left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_raw,f'{i}-skltn_right.csv'))\n",
    "\n",
    "    # Interpolatate Missing Data with IterativeImputer\n",
    "    mvmt_left = fill_gaps(mvmt_left)\n",
    "    mvmt_right = fill_gaps(mvmt_right)\n",
    "    skltn_left = fill_gaps(skltn_left)\n",
    "    skltn_right = fill_gaps(skltn_right)\n",
    "\n",
    "    # Save the raw data\n",
    "    mvmt_left.to_csv(os.path.join(save_imp,f'{i}-mvmt-left.csv'))\n",
    "    mvmt_right.to_csv(os.path.join(save_imp,f'{i}-mvmt-right.csv'))\n",
    "    skltn_left.to_csv(os.path.join(save_imp,f'{i}-skltn_left.csv'))\n",
    "    skltn_right.to_csv(os.path.join(save_imp,f'{i}-skltn_right.csv'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sktime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
